# -*- coding: utf-8 -*-
"""Copy of data_processing_pmdata.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1169B3Rj1t5jECxhy7sJ8BX2JoRz2OLDx
"""

from google.colab import drive
drive.mount('/content/drive')

import seaborn as sn
import matplotlib.pyplot as plt
import numpy as np

import zipfile

# Specify the path and filename of the ZIP file
ranjan_drive_folder_path = "/content/drive/MyDrive/Group-Akshat_NIkita_Ranjan/data/"
zip_file_path = f"{ranjan_drive_folder_path}archive.zip"
# Specify the directory where you want to extract the contents
extract_dir = "/content//data"

# Extract the contents of the ZIP file
with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:
    zip_ref.extractall(extract_dir)

"""##### Merging the files for a single person into single dataframe with aggregated sum values for simple dataframes except **sleep.json, sleep_score.json, exercise.json, resting_heart_rate.json, time_in_heart_rate.json, heart_rate.json**"""

import os
from tqdm import tqdm
import pandas as pd


first_df = True
final_df = pd.DataFrame()
folder_path = '/content/data/osfstorage-archive/pmdata/p01/fitbit/'
omitted_files = ["sleep.json", "sleep_score.csv", "exercise.json", "resting_heart_rate.json", "time_in_heart_rate_zones.json", "heart_rate.json"]
for file_name in tqdm(os.listdir(folder_path)):
  if file_name not in omitted_files:
    file_path = folder_path + file_name
    current_df = pd.read_json(file_path, encoding='utf-8-sig')
    current_df.rename(columns={'value': file_name[:-5]}, inplace=True)
    if len(current_df['dateTime'][0].__str__()) == 19:
      current_df['date'] = current_df['dateTime'].apply(lambda r: r.date().__str__())
      aggregated_df = current_df.groupby('date').sum()
    else:
      aggregated_df = current_df.rename(columns={'dateTime': 'date'})
    if first_df:
      first_df = False
      final_df = aggregated_df
    else:
      final_df = final_df.join(aggregated_df)

final_df

"""Checking omitted files one by one"""

print(omitted_files)

"""sleep.json"""

# repeating_dates = aggregated_df.groupby('date').count()[aggregated_df.groupby('date').count()['logId'] > 1].index.values.tolist()
# dates = aggregated_df['date']
# aggregated_df[dates.isin(repeating_dates)]

# sleep.json
file_path = folder_path + omitted_files[0]
current_df = pd.read_json(file_path, encoding='utf-8-sig')
aggregated_df = current_df.rename(columns={'dateOfSleep': 'date'})
aggregated_df.drop(['logId', 'startTime', 'endTime', 'levels'], axis=1, inplace=True)
aggregated_df = aggregated_df.groupby('date').sum()
temp_final_df = final_df[:]
temp_final_df.insert(0, 'date_l', final_df.index.astype(str), True)
final_df = pd.merge(temp_final_df, aggregated_df, left_on='date_l', right_on='date', how='left')

print(f"left_df columns: {len(temp_final_df.columns)}\n "+
      f"right_df columns: {len(aggregated_df.columns)}\n "+
      f"merged_df columns: {len(final_df.columns)}\n ")
final_df

"""sleep_score.csv"""

# sleep_score.csv
file_path = folder_path + omitted_files[1]
current_df = pd.read_csv(file_path, encoding='utf-8-sig')
current_df.insert(0, 'date', current_df['timestamp'].apply(lambda r: r.__str__().split("T")[0]), True)
current_df.drop('timestamp', axis=1, inplace=True)
temp_final_df = final_df[:]
if 'date_l' not in final_df.columns:
  temp_final_df.insert(0, 'date_l', final_df.index.astype(str), True)
final_df = pd.merge(temp_final_df, current_df, left_on='date_l', right_on='date', how='left')

print(f"left_df columns: {len(temp_final_df.columns)}\n "+
      f"right_df columns: {len(current_df.columns)}\n "+
      f"merged_df columns: {len(final_df.columns)}\n ")

final_df

"""exercise.json needs more understanding

# TODO
DONE 1. download the the exercise.json file and check it in Excel about startTime and originalStartTime.

DONE 2. originalDuration -> sum up value for the day

DONE 3. elelvationGain -> sum up value for the day

DONE 4. hasGps,	shouldFetchDetail, distanceUnit, tcxLink -> are not insightful for our work, so we will drop these.

DONE DROPPED 5. source -> to be checked what data is available, based on that to come to a strategy to combine.
6. distance, speed, pace, vo2Max -> check whether all the rows are NaN if so then drop else sum(distance), avg(speed, pace, vo2Max)

non null values

speed       25

pace        13

vo2Max      11

distance    25

as non null values are vary low these columns are dropped

dtype: int64
"""

# # exercise.json
file_path = folder_path + omitted_files[2]
excercise_df = pd.read_json(file_path, encoding='utf-8-sig')
temp_df = excercise_df[['startTime', 'originalStartTime', 'originalDuration', 'elevationGain',
       'hasGps', 'shouldFetchDetails', 'distance', 'distanceUnit', 'source',
       'tcxLink', 'speed', 'pace']]

temp_df = temp_df.drop(['originalStartTime','hasGps','shouldFetchDetails','distanceUnit','tcxLink', 'source'],axis=1)

current_df=temp_df[['originalDuration','startTime','elevationGain']]
current_df['date']=pd.to_datetime(current_df['startTime'])
current_df['date'] = current_df['date'].apply(lambda r: r.date().__str__())

aggregated_df = current_df.groupby('date').sum()

temp_final_df = final_df[:]
if 'date_l' not in final_df.columns:
  temp_final_df.insert(0, 'date_l', final_df.index.astype(str), True)
final_df = pd.merge(temp_final_df, aggregated_df, left_on='date_l', right_on='date', how='left')

print(f"left_df columns: {len(temp_final_df.columns)}\n "+
      f"right_df columns: {len(aggregated_df.columns)}\n "+
      f"merged_df columns: {len(final_df.columns)}\n ")

final_df

"""resting_heart_rate.json"""

# resting_heart_rate.json
file_path = folder_path + omitted_files[3]
current_df = pd.read_json(file_path, encoding='utf-8-sig')
current_df.insert(len(current_df.columns), 'resting_heart_rate', current_df['value'].apply(lambda r: r['value']), True)
current_df.insert(len(current_df.columns), 'resting_heart_rate_error', current_df['value'].apply(lambda r: r['error']), True)
current_df.drop('value', axis=1, inplace=True)
current_df.rename(columns={'dateTime': 'date'}, inplace=True)
current_df[['date']] = current_df[['date']].astype(str)
temp_final_df = final_df[:]
if 'date_l' not in final_df.columns:
  temp_final_df.insert(0, 'date_l', final_df.index.astype(str), True)
final_df = pd.merge(temp_final_df, current_df, left_on='date_l', right_on='date', how='left')

print(f"left_df columns: {len(temp_final_df.columns)}\n "+
      f"right_df columns: {len(current_df.columns)}\n "+
      f"merged_df columns: {len(final_df.columns)}\n ")

"""time_in_heart_rate_zones.json"""

# time_in_heart_rate_zones.json
file_path = folder_path + omitted_files[4]
current_df = pd.read_json(file_path, encoding='utf-8-sig')
current_df.insert(len(current_df.columns), 'BELOW_DEFAULT_ZONE_1', current_df['value'].apply(lambda r: r['valuesInZones']['BELOW_DEFAULT_ZONE_1']), True)
current_df.insert(len(current_df.columns), 'IN_DEFAULT_ZONE_1', current_df['value'].apply(lambda r: r['valuesInZones']['IN_DEFAULT_ZONE_1']), True)
current_df.insert(len(current_df.columns), 'IN_DEFAULT_ZONE_3', current_df['value'].apply(lambda r: r['valuesInZones']['IN_DEFAULT_ZONE_3']), True)
current_df.insert(len(current_df.columns), 'IN_DEFAULT_ZONE_2', current_df['value'].apply(lambda r: r['valuesInZones']['IN_DEFAULT_ZONE_2']), True)
current_df.drop('value', axis=1, inplace=True)
aggregated_df = current_df.rename(columns={'dateTime': 'date'})
aggregated_df[['date']] = aggregated_df[['date']].astype(str)
temp_final_df = final_df[:]
if 'date_l' not in final_df.columns:
  temp_final_df.insert(0, 'date_l', final_df.index.astype(str), True)
final_df = pd.merge(temp_final_df, aggregated_df, left_on='date_l', right_on='date', how='left')

print(f"left_df columns: {len(temp_final_df.columns)}\n "+
      f"right_df columns: {len(aggregated_df.columns)}\n "+
      f"merged_df columns: {len(final_df.columns)}\n ")

"""heart_rate.json"""

# heart_rate.json
file_path = folder_path + omitted_files[5]
current_df = pd.read_json(file_path, encoding='utf-8-sig')
current_df.insert(len(current_df.columns), 'bpm', current_df['value'].apply(lambda r: r['bpm']), True)
current_df.insert(len(current_df.columns), 'bpm_confidence', current_df['value'].apply(lambda r: r['confidence']), True)
current_df['date'] = current_df['dateTime'].apply(lambda r: r.date().__str__())
current_df.drop('value', axis=1, inplace=True)
aggregated_df = current_df.groupby('date').agg(pd.Series.mean)
aggregated_df.drop('dateTime', axis=1, inplace=True)
temp_final_df = final_df[:]
if 'date_l' not in final_df.columns:
  temp_final_df.insert(0, 'date_l', final_df.index.astype(str), True)
final_df = pd.merge(temp_final_df, aggregated_df, left_on='date_l', right_on='date', how='left')

print(f"left_df columns: {len(temp_final_df.columns)}\n "+
      f"right_df columns: {len(aggregated_df.columns)}\n "+
      f"merged_df columns: {len(final_df.columns)}\n ")

final_df

final_df.to_csv(f'{ranjan_drive_folder_path}merged_df3.csv', index=False)

"""soreness_area drop

readiness drop

sleep quality drop

mood drop

except sleep quality these are subjective thats why we are dropping
and sleep quality is alredy in dataset
"""

final_df





final_df.drop(['duration','minutesToFallAsleep','infoCode','date_x','sleep_log_entry_id','originalDuration','date_y','date','mainSleep'],axis=1,inplace=True)
final_df

corr_matrix = final_df.corr()
plt.figure(figsize=(25,25))
sn.heatmap(corr_matrix, annot=True)
plt.show()

final_df.to_csv(f'{ranjan_drive_folder_path}merged_df.csv', index=False)

